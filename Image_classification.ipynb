{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import cv2 as cv\n",
    "from imageai.Detection import ObjectDetection\n",
    "import os\n",
    "import random, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    tf.test.gpu_device_name()\n",
    "    # need to use gpu runtime accelerator!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/User/Documents/GitHub/sidewalks_deep-learning\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(f'./images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/User/Documents/GitHub/sidewalks_deep-learning/images\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: './sidewalk'\n",
      "C:\\Users\\User\\Documents\\GitHub\\sidewalks_deep-learning\\images\\sidewalk\n",
      "C:\\Users\\User\\Documents\\GitHub\\sidewalks_deep-learning\\images\n",
      "C:\\Users\\User\\Documents\\GitHub\\sidewalks_deep-learning\\images\\no_sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter format not correct - \"sidewalk\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Documents\\GitHub\\sidewalks_deep-learning\\images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid switch - \"no_sidewalk\".\n"
     ]
    }
   ],
   "source": [
    "#move images into train, test, validate folders with subfolders for classes\n",
    "test_ratio = .15\n",
    "SPLIT_DATA = True # switch if need to divide out data\n",
    "if SPLIT_DATA:\n",
    "    for label in ['sidewalk', 'no_sidewalk']:\n",
    "        %cd ./$label\n",
    "        listing = os.popen('ls').read().strip().split(sep='\\n')\n",
    "        random.shuffle(listing) #inplace\n",
    "        im_count = len(listing)\n",
    "        test_size=val_size = math.ceil(test_ratio*im_count)\n",
    "        train_size = im_count - (test_size + val_size)\n",
    "        subfolders = ['train', 'test', 'vdate']\n",
    "        for subfolder in subfolders:\n",
    "            os.makedirs(os.path.join(os.path.dirname(os.getcwd()), subfolder, label))\n",
    "        for item in range(0, train_size):\n",
    "            moved = listing.pop()\n",
    "            !mv $moved ../train/$label/$moved\n",
    "        for item in range(0, test_size):\n",
    "            moved = listing.pop()\n",
    "            !mv $moved ../test/$label/$moved\n",
    "        for item in range(0, val_size):\n",
    "            moved = listing.pop()\n",
    "            !mv $moved ../vdate/$label/$moved\n",
    "        %cd ..\n",
    "        !rmdir ./$label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for data loading and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "idg_params={'rescale': 1./255}\n",
    "augmentation_params = {'rotation_range': 30,\n",
    "                        'zoom_range': 0.15,\n",
    "                         'width_shift_range': 0.2,\n",
    "                         'height_shift_range': 0.2,\n",
    "                         'shear_range': 0.15,\n",
    "                         'fill_mode': 'nearest',\n",
    "                         'horizontal_flip': True}\n",
    "datagen_params = {'batch_size': 32,\n",
    "                  'target_size': (256,256),\n",
    "                  'color_mode': 'rgb', \n",
    "                  'class_mode': 'binary'}\n",
    "batch_size = 128\n",
    "n_epochs = 15\n",
    "IMG_HEIGHT = 200\n",
    "IMG_WIDTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(**idg_params)\n",
    "train_augmented_datagen = ImageDataGenerator(\n",
    "    **idg_params,\n",
    "    **augmentation_params)\n",
    "test_datagen = ImageDataGenerator(**idg_params)\n",
    "vdate_datagen = ImageDataGenerator(**idg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2087 images belonging to 2 classes.\n",
      "Found 448 images belonging to 2 classes.\n",
      "Found 448 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        r'./train/',  # This is the source directory for training images\n",
    "        **datagen_params)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        r'./test/',\n",
    "        **datagen_params)\n",
    "vdate_generator = vdate_datagen.flow_from_directory(\n",
    "        r'./vdate/',\n",
    "        **datagen_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator based caluculations\n",
    "input_shape = train_generator.next()[0].shape[1:]\n",
    "total_train_samples = train_generator.n\n",
    "total_vdate_samples = vdate_generator.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 253, 253, 32)      1568      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 251, 251, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 500000)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                32000064  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 32,011,010\n",
      "Trainable params: 32,011,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model = tf.keras.models.Sequential()\n",
    "conv_model.add(Conv2D(32, kernel_size=(4, 4),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "conv_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "conv_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "conv_model.add(Dropout(0.20))\n",
    "conv_model.add(Flatten())\n",
    "conv_model.add(Dense(64, activation='relu'))\n",
    "conv_model.add(Dropout(0.2))\n",
    "conv_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "conv_model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_history = conv_model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch=int(total_train_samples/batch_size),  \n",
    "        epochs=n_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=vdate_datagen,\n",
    "        validation_steps=int(total_vdate_samples/batch_size),\n",
    "        use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "n_epochs_range = np.arange(0, n_epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "fig, ax = plt.figure()\n",
    "ax.plot(n_epochs_range, conv_history.history[\"loss\"], label=\"train_loss\")\n",
    "ax.plot(n_epochs_range, conv_history.history[\"val_loss\"], label=\"val_loss\")\n",
    "ax.plot(n_epochs_range, conv_history.history[\"acc\"], label=\"train_acc\")\n",
    "ax.plot(n_epochs_range, conv_history.history[\"val_acc\"], label=\"val_acc\")\n",
    "\n",
    "ax.set_title(\"Training Loss and Accuracy on Dataset\")\n",
    "ax.set_xlabel(\"Epoch #\")\n",
    "ax.set_ylabel(\"Loss/Accuracy\")\n",
    "ax.set_legend(loc=\"lower left\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection via Transfer Learning\n",
    "This will use a pretrained model as the basis for a nn that identifies the part of an image file that contains the sidewalk object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "loss = 'binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_mobilenet = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=input_shape,\n",
    "    include_top=False,\n",
    "    weights='imagenet')\n",
    "pretrained_mobilenet.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_pooling_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = keras.layers.Dense(1)\n",
    "\n",
    "mnet_model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_pooling_layer,\n",
    "  prediction_layer])\n",
    "\n",
    "mnet_model.compile(optimizer=RMSprop(lr=learning_rate),\n",
    "              loss=loss,\n",
    "              metrics=['accuracy'])\n",
    "mnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnet_history = mnet_model.fit_generator(\n",
    "        train_augmented_generator, \n",
    "        steps_per_epoch=int(total_train_samples/batch_size),  \n",
    "        epochs=n_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=vdate_datagen\n",
    "        validation_steps=int(total_vdate_samples/batch_size),\n",
    "        use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/yolo-tiny.h5\"\n",
    "input_path = \"./input/test45.jpg\"\n",
    "output_path = \"./output/newimage.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tinyYOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsTinyYOLOv3()\n",
    "detector.setModelPath(model_path)\n",
    "detector.loadModel()\n",
    "\n",
    "#returns dictionary with name and probabilities of objects detected\n",
    "detection = detector.detectObjectsFromImage(input_image=input_path, output_image_path=output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
