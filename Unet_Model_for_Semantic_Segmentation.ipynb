{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Y5Eyt_8WTMg"
   },
   "source": [
    "# Using U-net style architecture for semantic segmentation\n",
    "I will train a u-net-style architecture on the labeled city dataset from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27pX0dRKUlUq"
   },
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rQVzCt6XUjam"
   },
   "outputs": [],
   "source": [
    "#centralized location for params needing tweaked\n",
    "batch_size = 4\n",
    "target_size = (512,256)\n",
    "n_epochs = 25\n",
    "learning_rate = 0.000_001\n",
    "run_once, fix_masks = False, True # to load/unzip/fix data, fix masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_1OrbHAMN5k"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RINwHYjearUG",
    "outputId": "0a49c84e-386d-44ad-d8af-54287e3c83a6"
   },
   "outputs": [],
   "source": [
    "# get colab status\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  %tensorflow_version 2.x\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-qrlIDKa8x4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras import metrics \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import (Dense, Conv2D, Flatten, Dropout, \n",
    "MaxPooling2D, BatchNormalization, Conv2DTranspose, concatenate, Input)\n",
    "from tensorflow.keras.utils import Sequence \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence # to fix 'imagedatagenerator has no shape' error\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os, zipfile, glob, shutil\n",
    "from math import ceil \n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "import imageio.core.util  # so can be patched\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "id": "Mg3xJNubNYXZ",
    "outputId": "b734addf-6c7c-4803-8659-114fec261dff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/User/Google Drive/thinkful/colab_datasets/sidewalk_data\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.chdir(r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/')\n",
    "else:\n",
    "    os.chdir(os.path.expanduser(r'~/Google Drive/thinkful/colab_datasets/sidewalk_data/'))\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUZmgziJcCCv"
   },
   "source": [
    "# Data Load\n",
    "### Cityscape Dataset\n",
    "This is a publically availible dataset from https://www.cityscapes-dataset.com/, though not downloadable without requesting a login which is granted based on email domain.\n",
    "Citation:  \n",
    ">Cvpr2016M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes Dataset for Semantic Urban Scene Understanding,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [Bibtex]\n",
    "[main paper](https://www.cityscapes-dataset.com/wordpress/wp-content/papercite-data/pdf/cordts2016cityscapes.pdf) · [supplemental ](https://www.cityscapes-dataset.com/wordpress/wp-content/papercite-data/pdf/cordts2016cityscapes-supplemental.pdf)· [arxiv](http://arxiv.org/abs/1604.01685) · [CVF](http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BmO_yxT5cBzp"
   },
   "outputs": [],
   "source": [
    "if run_once: # run once\n",
    "  os.chdir(r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/')\n",
    "  with zipfile.ZipFile(os.path.join(os.getcwd(), 'gtFine_trainvaltest.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./cityscape/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZyo7fHrNvLU"
   },
   "outputs": [],
   "source": [
    "#Color for sidewalk in mask from cityscape website:\n",
    "sidewalk_color = np.array([244, 35, 232])\n",
    "sidewalk_colorl = [244, 35, 232]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6fawQA4DC0e"
   },
   "source": [
    "### downsample images and fix directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-I3mnGdj24R"
   },
   "outputs": [],
   "source": [
    "def color_checker(color):\n",
    "    '''looks for color used to code sidewalks'''\n",
    "    return 1 if np.array_equal(color, sidewalk_color) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Uo1u5iaDCUX"
   },
   "outputs": [],
   "source": [
    "def resize(path, destination):\n",
    "    '''\n",
    "    downsample images by 4x, \n",
    "    remove transparency layer, \n",
    "    save to flattened dir stuc\n",
    "    Args:\n",
    "        path = current directory\n",
    "        destination = destination for images\n",
    "    '''\n",
    "    dirs1 = np.array([os.path.join(path, x) for x in os.listdir(path)])\n",
    "    dirs = dirs1[[os.path.isdir(x) for x in dirs1]]\n",
    "    for direc in dirs:   # direc should be mask vs image folders\n",
    "        if not os.path.isdir(direc):\n",
    "            continue\n",
    "        subdirs = [os.path.join(direc, x) for x in os.listdir(direc)]\n",
    "        for subdir in subdirs:   # subdir should be train, val, test\n",
    "            subdirs2 = [os.path.join(subdir, x) for x in os.listdir(subdir)]\n",
    "            for subdir2 in subdirs2:   # subdir2 is city folders\n",
    "                items = os.listdir(subdir2)\n",
    "                for item in items:\n",
    "                    if not os.path.isfile(os.path.join(subdir2,item)):\n",
    "                        continue\n",
    "                    #skip json and noncolor masks\n",
    "                    if item.rfind('gtFine_color.png') == -1 \\\n",
    "                    and item.rfind('leftImg8bit.png') == -1:\n",
    "                        continue\n",
    "                    #if item.rfind('leftImg8bit.png') != -1:  # remove if doing images\n",
    "                    #    continue\n",
    "                    destin2 = os.path.join(destination, subdir[len(common_path)+1:])\n",
    "                    f, _ = os.path.splitext(item)\n",
    "                    if not os.path.exists(destin2):\n",
    "                        os.makedirs(destin2)\n",
    "                    destin3 = os.path.join(destin2, f+'_resized.png')\n",
    "                    ###########WARNING\n",
    "                    #ADDED transfomation for masks. IF running \n",
    "                    #from scratch, add logic to treat masks vs images separately\n",
    "                    ###############\n",
    "                    #filename = os.path.join(subdir2, item)\n",
    "                    #im = io.imread(filename)\n",
    "                    #im = im[:,:,0:3]  # remove alpha channel\n",
    "                    #reduced_im = np.apply_along_axis(color_checker, 2, im).astype(np.intc)\n",
    "                    #reduced_im = transform.resize(reduced_im, \n",
    "                    #                            (256,512), \n",
    "                    #                            preserve_range=True, \n",
    "                    #                            anti_aliasing=False).astype(np.bool_).astype(np.intc)\n",
    "                    #io.imsave(destin3, reduced_im) \n",
    "\n",
    "\n",
    "\n",
    "                    im = Image.open(os.path.join(subdir2, item))\n",
    "                    if im.mode == \"RGBA\":\n",
    "                        im = im.convert('RGB')\n",
    "                    im = im.resize((512,256), Image.BICUBIC)\n",
    "                    im.save(destin3, format='PNG')\n",
    "                    im.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hjQJpIMvdxud",
    "outputId": "3cf158b6-6d8a-4047-c82e-a7b49e50557b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/User/Google Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape\n"
     ]
    }
   ],
   "source": [
    "#switch to cloud dir\n",
    "if IN_COLAB:\n",
    "    data_mount = r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/'  \n",
    "    destin = r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/'\n",
    "else:\n",
    "    data_mount = r'C:\\Users\\User\\Google Drive\\thinkful\\colab_datasets\\sidewalk_data\\public_data'\n",
    "    destin = r\"C:\\Users\\User\\Google Drive\\thinkful\\colab_datasets\\sidewalk_data\\public_data\\cityscape_reduced\"\n",
    "leftimages_path = os.path.join(data_mount, r'cityscape/leftImg8bit/') # no leading /\n",
    "mask_path = os.path.join(data_mount, r'cityscape/gtFine/')\n",
    "common_path = os.path.join(data_mount, r'cityscape')\n",
    "image_dest_path = os.path.join(destin, r'leftImg8bit')\n",
    "mask_dest_path = os.path.join(destin, r'sidewalk_only_masks')\n",
    "os.chdir(common_path)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeFhHYZnDy8M"
   },
   "outputs": [],
   "source": [
    "if run_once:\n",
    "  resize(common_path, destin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1kRBlCrapN1"
   },
   "source": [
    "## Adjust masks to only separate sidewalk vs non-sidewalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gYJdfnIAtPqW",
    "outputId": "745375d0-6f52-4e4e-9cbb-63fc48fca6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/User/Google Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O87DXqZVL201"
   },
   "outputs": [],
   "source": [
    "#Requires both patching library and regular filterwarnings to avoid \n",
    "#warnings about low resolution!\n",
    "def silence_imageio_warning(*args, **kwargs):\n",
    "    pass\n",
    "imageio.core.util._precision_warn = silence_imageio_warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mSpwws8UaRK"
   },
   "source": [
    "#### create dataframe of mask files to operate on and intended output filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19SCyIIcXBPp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "WW3r9aqRUBx0",
    "outputId": "05c70e6b-645b-46a8-a3a4-b7e3fda484fc"
   },
   "outputs": [],
   "source": [
    "os.chdir(mask_path)\n",
    "if fix_masks:\n",
    "    file_list = ! find . -name \"*_gtFine_color.png\"\n",
    "    mask_fixes_df = pd.DataFrame([path.split(\"/\") for path in file_list])\n",
    "    mask_fixes_df.columns = ['dot', 'subset', 'city', 'filename']\n",
    "    mask_fixes_df['dest_path'] = destin + r'/sidewalk_only_masks/' + \\\n",
    "    mask_fixes_df.subset + '/folder0/' + mask_fixes_df.filename\n",
    "    mask_fixes_df['old_path'] = file_list\n",
    "    mask_fixes_df['old_path'] = mask_fixes_df['old_path'].apply(lambda x: mask_path + x[2:])\n",
    "    mask_fixes_df = mask_fixes_df[mask_fixes_df.subset != 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QF8Xz-lXMRth"
   },
   "outputs": [],
   "source": [
    "def remove_extant(path):\n",
    "    return True if os.path.exists(path) else False\n",
    "mask_fixes_df['already_fixed'] = mask_fixes_df.dest_path.apply(remove_extant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "gxV5ph9pP10m",
    "outputId": "fef03034-7194-4d07-8e59-4b18a2d792ca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dot</th>\n",
       "      <th>subset</th>\n",
       "      <th>city</th>\n",
       "      <th>filename</th>\n",
       "      <th>dest_path</th>\n",
       "      <th>old_path</th>\n",
       "      <th>already_fixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>.</td>\n",
       "      <td>train</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aachen_000000_000019_gtFine_color.png</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>.</td>\n",
       "      <td>train</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aachen_000001_000019_gtFine_color.png</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>.</td>\n",
       "      <td>train</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aachen_000002_000019_gtFine_color.png</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>.</td>\n",
       "      <td>train</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aachen_000003_000019_gtFine_color.png</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>.</td>\n",
       "      <td>train</td>\n",
       "      <td>aachen</td>\n",
       "      <td>aachen_000004_000019_gtFine_color.png</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>C:\\Users\\User\\Google Drive\\thinkful\\colab_data...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dot subset    city                               filename  \\\n",
       "1488   .  train  aachen  aachen_000000_000019_gtFine_color.png   \n",
       "1489   .  train  aachen  aachen_000001_000019_gtFine_color.png   \n",
       "1490   .  train  aachen  aachen_000002_000019_gtFine_color.png   \n",
       "1491   .  train  aachen  aachen_000003_000019_gtFine_color.png   \n",
       "1492   .  train  aachen  aachen_000004_000019_gtFine_color.png   \n",
       "\n",
       "                                              dest_path  \\\n",
       "1488  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...   \n",
       "1489  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...   \n",
       "1490  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...   \n",
       "1491  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...   \n",
       "1492  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...   \n",
       "\n",
       "                                               old_path  already_fixed  \n",
       "1488  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...          False  \n",
       "1489  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...          False  \n",
       "1490  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...          False  \n",
       "1491  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...          False  \n",
       "1492  C:\\Users\\User\\Google Drive\\thinkful\\colab_data...          False  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_fixes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-rQEvT9XQmh"
   },
   "outputs": [],
   "source": [
    "def mask_reduce(old_file, new_file):\n",
    "    '''reduces images from rgb to b/w mask'''\n",
    "    im = io.imread(old_file)\n",
    "    im = im[:,:,0:3]  # remove alpha channel\n",
    "    #reduced_im = np.apply_along_axis(color_checker, 2, im).astype(np.intc)\n",
    "    reduced_im = (im == sidewalk_colorl).all(-1)  # SO. MUCH. FASTER.\n",
    "    reduced_im = transform.resize(reduced_im, \n",
    "                                  (256,512), \n",
    "                                  preserve_range=True, \n",
    "                                  anti_aliasing=False).astype(np.bool_).astype(np.intc)\n",
    "    #all black images throw error\n",
    "    try:\n",
    "        io.imsave(new_file, reduced_im) \n",
    "    except ValueError:\n",
    "        io.imsave(new_file, reduced_im.astype('float64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnUgvVX-VOok"
   },
   "source": [
    "### find masks not yet created and do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I--D9TvUlhdX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if fix_masks:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        warnings.filterwarnings('ignore')\n",
    "        #apply fix to mask file w/o extant target\n",
    "        mask_fixes_df[mask_fixes_df.already_fixed==False]\\\n",
    "            .apply(lambda x: mask_reduce(x.old_path, x.dest_path), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NIbz-k-aIpt-"
   },
   "outputs": [],
   "source": [
    "os.chdir(destin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkj0QI8NWybp"
   },
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QsDAxU1dZwR"
   },
   "outputs": [],
   "source": [
    "def get_image_locs(subset, im_path, add_folder0):    \n",
    "    '''grab name of every image \n",
    "    and store in dataframe with subset eg. train, test'''\n",
    "    df = pd.DataFrame(columns=['img_name', 'subset'])\n",
    "    os.chdir(im_path)   # needed despite lack of reference to curpath\n",
    "    img_paths = os.listdir(im_path) \n",
    "    if subset == 'img':\n",
    "        search_for = '*/*'\n",
    "    elif subset == 'mask':   \n",
    "        search_for = '*/*_gtFine_color.png'\n",
    "    else:\n",
    "        raise NameError(\"subset should be 'img' or 'mask'\")\n",
    "    if add_folder0:\n",
    "        im_paths = [os.path.join(im, 'folder0') for im in im_paths]\n",
    "    for subsetdir in img_paths:\n",
    "        img_files = [x[len(subsetdir)+1:] for x in glob.glob(os.path.join(subsetdir, search_for))]\n",
    "        img_files_s = pd.Series(img_files)\n",
    "        new_df = None\n",
    "        new_df = pd.DataFrame(columns=['img_name', 'subset'])\n",
    "        new_df['img_name'] = img_files_s\n",
    "        new_df['subset'].fillna(subsetdir, inplace=True)\n",
    "        df = df.append(new_df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkAAKtaSQTMX"
   },
   "outputs": [],
   "source": [
    "def clean_name(name):\n",
    "  '''removes _gtFine_color.png or _leftImg8bit.png'''\n",
    "  which = name.rfind('_leftImg8bit.png')\n",
    "  if which == -1:\n",
    "    which = '_gtFine_color.png'\n",
    "  else:\n",
    "    which =  '_leftImg8bit.png'\n",
    "  loc=name.rfind(which)\n",
    "  return name[:loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArIMjgFz-uz_"
   },
   "outputs": [],
   "source": [
    "data_df = get_image_locs('img', leftimages_path, False)\n",
    "mask_df = get_image_locs('mask', mask_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSnp4W4JPmNS"
   },
   "outputs": [],
   "source": [
    "data_df['common_name'] = data_df.img_name.apply(clean_name)\n",
    "mask_df['common_name'] = mask_df.img_name.apply(clean_name)\n",
    "data_full_df = None\n",
    "data_full_df = pd.merge(left=data_df, right=mask_df, \n",
    "                        how='inner', on='common_name',\n",
    "                        sort=False, \n",
    "                        suffixes=('_data', '_mask'))\\\n",
    "               [['img_name_data', 'subset_data', 'img_name_mask']]\n",
    "data_full_df.columns = ['img_name', 'subset_data', 'mask_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zpvr4AWeWyIk"
   },
   "outputs": [],
   "source": [
    "class ImgLoadGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    parent Sequence object generates data batches\n",
    "    \"\"\"\n",
    "    def __init__(self, data, batch_size, data_path, subset, widthxheight): \n",
    "        self.data = data[data.subset_data == subset]\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_list = data.img_name.to_list()\n",
    "        self.mask_list = data.mask_name.to_list()\n",
    "        self.subset = subset\n",
    "        self.w_h = widthxheight\n",
    "       \n",
    "    def __len__(self):\n",
    "      \"\"\"last batch usually smaller\"\"\"\n",
    "      return int(np.ceil(len(self.image_list) / float(self.batch_size)))\n",
    "    def decode_img(self, file_path):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        # convert the compressed string to a 3D uint8 tensor\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)  # to [0,1] rng\n",
    "        return tf.image.resize(img, self.w_h)\n",
    "    def process_path(self, file_path):\n",
    "        # load the raw data from the file as a string\n",
    "\n",
    "        return img, label\n",
    "    def get_batch(self, idx, path_list, img_or_mask):\n",
    "        # Fetch a batch of images from a list of paths\n",
    "        if img_or_mask == 'img':\n",
    "            path_l = os.path.join(self.data_path, \"leftImg8bit\", self.subset)\n",
    "            col_slice = 0\n",
    "        else:\n",
    "            path_l = os.path.join(self.data_path, \"gtFine\", self.subset)\n",
    "            col_slice = 2\n",
    "        slice_start, slice_end = idx * self.batch_size, (1 + idx) * self.batch_size\n",
    "        im_set = self.data.iloc[slice_start:slice_end,col_slice].values\n",
    "        return np.array([self.decode_img(os.path.join(path_l, im)) for im in im_set])\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.get_batch(idx, self.image_list, 'img')\n",
    "        batch_y = self.get_batch(idx, self.mask_list, 'mask')\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AzJ1S1Fbl-S"
   },
   "outputs": [],
   "source": [
    "gen_params = {'data':data_full_df,\n",
    "              'batch_size':8, \n",
    "              'data_path':common_path, \n",
    "              'widthxheight':target_size}\n",
    "train_generator = ImgLoadGenerator(subset='train', \n",
    "                                   **gen_params)\n",
    "vdate_generator = ImgLoadGenerator(subset='val',\n",
    "                                   **gen_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yinMtAVmKKIW"
   },
   "source": [
    "### ImageDataGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "-v8NXoyPahUq",
    "outputId": "413c3039-452e-4de9-99de-172ed99dfe6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/train/folder0\n",
      "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/leftImg8bit/train/folder0\n",
      "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/test/folder0\n",
      "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/leftImg8bit/test/folder0\n",
      "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/val/folder0\n",
      "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/leftImg8bit/val/folder0\n"
     ]
    }
   ],
   "source": [
    "#create some subfolders for flow_for_directory\n",
    "subset = ['train', 'test', 'val']\n",
    "pathset = [mask_dest_path, image_dest_path]\n",
    "for sub in subset:\n",
    "    for path in pathset:\n",
    "        new_path = os.path.join(path, sub, 'folder0')\n",
    "        if not os.path.exists(new_path):\n",
    "            os.mkdir(new_path)\n",
    "        file_listing = os.listdir(os.path.join(path,sub))\n",
    "        file_listing = [file for file in file_listing if file!=new_path]        \n",
    "        for file in file_listing:\n",
    "            src=os.path.join(path,sub,file)\n",
    "            dst=os.path.join(new_path,file)\n",
    "            try:\n",
    "                shutil.move(src,dst)\n",
    "            except OSError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rkiEpShXY90w",
    "outputId": "3c8e940a-231b-4be1-c408-18885473aa48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/train'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(mask_dest_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-KTOE105cID"
   },
   "outputs": [],
   "source": [
    "#check all images have masks and vice-versa\n",
    "def check_im_mask_overlap(im_path, mask_path):\n",
    "    '''\n",
    "    Args:\n",
    "        how: pd.merge how=, as str. \n",
    "        inner for mask/image matchs, left for images WO mask, right mask WO img\n",
    "    '''\n",
    "    data_dest_df_img = get_image_locs('img', im_path, False)\n",
    "    mask_dest_df_mask = get_image_locs('mask', mask_path, False)\n",
    "    #print(data_dest_df_img[data_dest_df_img.duplicated(subset='img_name', keep='first')])\n",
    "    #print(mask_dest_df_mask[mask_dest_df_mask.duplicated(subset='img_name', keep='first')])\n",
    "    data_dest_df, mask_dest_df = pd.DataFrame(), pd.DataFrame()\n",
    "    data_dest_df['common_name'] = data_df.img_name.apply(clean_name)\n",
    "    mask_dest_df['common_name'] = mask_df.img_name.apply(clean_name)\n",
    "    data_full_df = None\n",
    "    data_full_df = pd.merge(left=data_df, right=mask_df, \n",
    "                            how='outer', on='common_name',\n",
    "                            indicator=True, sort=False, \n",
    "                            suffixes=('_data', '_mask'))\\\n",
    "                [['img_name_data', 'subset_data', 'img_name_mask', '_merge']]\n",
    "    data_full_df.columns = ['img_name', 'subset_data', 'mask_name', '_merge']\n",
    "    return data_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_JeKjS2saGN"
   },
   "source": [
    "### combined generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eio4oJwQsSg2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "4-PcSznXcFG7",
    "outputId": "faaf22cb-8916-4a1e-b243-1a686a2220ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 images belonging to 1 classes.\n",
      "Found 352 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "image_datagen = ImageDataGenerator(rescale=1./255)\n",
    "mask_datagen = ImageDataGenerator(rescale=1./255)\n",
    "seed=42\n",
    "ffd_params = dict(class_mode=None,\n",
    "                  target_size=target_size,\n",
    "                  seed=seed,\n",
    "                  batch_size=batch_size,\n",
    "                  )\n",
    "image_generator = image_datagen.flow_from_directory(\n",
    "    os.path.join(image_dest_path, r'train'),\n",
    "    **ffd_params)\n",
    "\n",
    "mask_generator = mask_datagen.flow_from_directory(\n",
    "    os.path.join(mask_dest_path, r'train'),\n",
    "    color_mode='grayscale',\n",
    "    **ffd_params)\n",
    "\n",
    "# combine generators into one which yields image and masks\n",
    "def combine_generator(gen1, gen2):\n",
    "    while True:\n",
    "        yield(next(gen1), next(gen2))\n",
    "\n",
    "train_generator_ffd = combine_generator(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Z-vruN0FuVM"
   },
   "outputs": [],
   "source": [
    "## Using Sequence to combine generators\n",
    "class MergedGenerators(Sequence):\n",
    "    def __init__(self, *generators):\n",
    "        self.generators = generators\n",
    "        # TODO add a check to verify that all generators have the same length\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.generators[0]) / float(self.batch_size)))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [generator[index] for generator in self.generators]\n",
    "    \n",
    "    def \n",
    "    \n",
    "\n",
    "datagen_args = dict(class_mode=None,\n",
    "                  target_size=target_size,\n",
    "                  seed=seed,\n",
    "                  batch_size=batch_size,\n",
    "                  )\n",
    "\n",
    "train_merged_generator = MergedGenerators(image_generator, mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "colab_type": "code",
    "id": "BcdEvxeUFc_B",
    "outputId": "de03420a-31cc-4e35-91d9-235d3406e1ad"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-f6e2d7c91dd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_merged_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'MergedGenerators' object is not callable"
     ]
    }
   ],
   "source": [
    "train_merged_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZj1nOBdcGI-"
   },
   "source": [
    "# Modeling\n",
    "I will build a basic U-net model, train it on the cityscape public dataset, and use that model & weights on the Denver dataset I have built. \n",
    "\n",
    "Model structure reference [here](http://cs230.stanford.edu/files_winter_2018/projects/6937642.pdf)\n",
    "\n",
    "Next, I will use the frozen weights from the first have of the model, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l8BuU1NmvgM"
   },
   "source": [
    "###Build Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKysTZrrmz0X"
   },
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "def history_plotter(history, n_epochs, plot_validation=True): \n",
    "    '''plot the training loss and accuracy\n",
    "    plot_validation exists because valdation not working in model\n",
    "    '''\n",
    "    n_epochs_range = np.arange(0, n_epochs)\n",
    "    plt.style.use(\"ggplot\")\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True,\n",
    "                                  figsize=(10,5))\n",
    "    ax1.plot(n_epochs_range, history.history[\"loss\"], label=\"Loss\")\n",
    "    ax1.plot(n_epochs_range, history.history[\"accuracy\"], label=\"Accuracy\")\n",
    "    ax1.plot(n_epochs_range, history.history['auc'], label='Area Under ROC')\n",
    "    ax1.plot(n_epochs_range, history.history['matthews_correlation'], label=\"Matthews Coef\")\n",
    "    if plot_validation:\n",
    "        ax2.plot(n_epochs_range, history.history[\"val_loss\"], label=\"loss\")\n",
    "        ax2.plot(n_epochs_range, history.history[\"val_accuracy\"], label=\"Accuracy\")\n",
    "        ax2.plot(n_epochs_range, history.history[\"val_auc\"], label=\"Area Under ROC\")\n",
    "        ax2.plot(n_epochs_range, history.history[\"val_matthews_correlation\"], label=\"Matthews Coef\")\n",
    "    \n",
    "    ax1.set_title(\"Training Metrics\")\n",
    "    ax2.set_title(\"Validation Metrics\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    ax1.set_ylabel(\"Metric Score\")\n",
    "    ax1.legend(); ax2.legend()\n",
    "    plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fRThv5kTFGT"
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gynMScrP_TBW"
   },
   "outputs": [],
   "source": [
    "img_shape = (*target_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h-Vg7YryWNue"
   },
   "outputs": [],
   "source": [
    "#use function for the blocks of the network\n",
    "def encoder_builder(input_, filters,\n",
    "                         activ='relu', kernel=(3,3), \n",
    "                         drop=.5, pad='same', kern_init='he_uniform'):\n",
    "  kwargs = {'filters': filters, 'activation': activ, 'kernel_size': kernel, \n",
    "       'padding': pad, 'kernel_initializer': kern_init}\n",
    "  x = Conv2D(**kwargs)(input_)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(drop)(x)\n",
    "  x = Conv2D(**kwargs)(x)\n",
    "  encoder = Dropout(drop)(x)\n",
    "  pooled = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(encoder)\n",
    "  return encoder, pooled\n",
    "\n",
    "def decoder_builder(input_, skip, filters, \n",
    "                    activ='relu', kernel=(2,2),\n",
    "                    drop=.5, pad='same', kern_init='he_uniform',\n",
    "                    ):\n",
    "  kwargs = {'filters': filters, 'activation': activ, 'kernel_size': kernel, \n",
    "       'padding': pad, 'kernel_initializer': kern_init} \n",
    "  x = Conv2DTranspose(**kwargs, strides=(2,2))(input_)\n",
    "  x = concatenate([x, skip], axis=-1)  # note axis is *-*1\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(drop)(x)\n",
    "  x = Conv2D(**kwargs)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(drop)(x)\n",
    "  x = Conv2D(**kwargs)(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqVLXyW3x94R"
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=img_shape)\n",
    "encoder1, pooled1 = encoder_builder(input_layer, filters=16)  # return (128x128x32)\n",
    "encoder2, pooled2 = encoder_builder(pooled1, filters=32)  # return (64x64x364)\n",
    "encoder3, pooled3 = encoder_builder(pooled2, filters=64)  # return (32x32x128)\n",
    "encoder4, pooled4 = encoder_builder(pooled3, filters=128)  # return (16x16x256)\n",
    "#encoder_builder(pooled4, filters=512)  # return (8x8x512)\n",
    "middle, middle_pool = encoder_builder(pooled4, filters=128)  # return (4x4x1024)\n",
    "#decoder512 = decoder_builder(middle, skip=encoder5, filters=512)\n",
    "decoder256 = decoder_builder(middle, skip=encoder4, filters=128)\n",
    "decoder128 = decoder_builder(decoder256, skip=encoder3, filters=64)\n",
    "decoder64 = decoder_builder(decoder128, skip=encoder2, filters=32)\n",
    "decoder32 = decoder_builder(decoder64, skip=encoder1, filters=16)\n",
    "out_layer = Conv2D(filters=1, kernel_size=(1, 1), \n",
    "                   activation='sigmoid')(decoder32)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6xAW0lHDuCw"
   },
   "outputs": [],
   "source": [
    "unet = models.Model(inputs=[input_layer], outputs=[out_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWDSkQxsa-gr"
   },
   "outputs": [],
   "source": [
    "plot_model(unet, show_shapes=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dorlRsdJlr2T"
   },
   "source": [
    "### Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k1ICDJMRUGj-"
   },
   "outputs": [],
   "source": [
    "input_shape = img_shape\n",
    "total_train_samples = data_full_df[data_full_df.subset_data == 'train'].shape[0]\n",
    "total_vdate_samples = data_full_df[data_full_df.subset_data == 'val'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Whoa9jSmM0Jj"
   },
   "outputs": [],
   "source": [
    "unet_model_path = r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/models/unet_weights.hdf5'\n",
    "cb_params = dict(monitor='val_mean_io_u_2', \n",
    "                 save_best_only=True, \n",
    "                 verbose=1)\n",
    "\n",
    "unet_cb = ModelCheckpoint(filepath=unet_model_path, **cb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NsRDy7X_EzUu",
    "outputId": "8b523938-c6dd-4c2f-ac9c-0acaaa050426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 512, 256, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 512, 256, 16) 64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512, 256, 16) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 256, 16) 2320        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 256, 16) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 256, 128, 16) 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 128, 32) 4640        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 128, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256, 128, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 256, 128, 32) 9248        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256, 128, 32) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 64, 32)  0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 64, 64)  18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 64, 64)  256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128, 64, 64)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 64, 64)  36928       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128, 64, 64)  0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 32, 64)   0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 32, 128)  73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 32, 128)  512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 64, 32, 128)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 32, 128)  147584      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 64, 32, 128)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 16, 128)  0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 16, 128)  147584      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 16, 128)  512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 16, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 16, 128)  147584      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 16, 128)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 64, 32, 128)  65664       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 64, 32, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 32, 256)  1024        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 64, 32, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 32, 128)  131200      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 32, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 64, 32, 128)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 32, 128)  65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 128, 64, 64)  32832       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 64, 128) 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 64, 128) 512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128, 64, 128) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 64, 64)  32832       dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128, 64, 64)  256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 128, 64, 64)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 64, 64)  16448       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 256, 128, 32) 8224        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256, 128, 64) 0           conv2d_transpose_2[0][0]         \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 256, 128, 64) 256         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 256, 128, 64) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 128, 32) 8224        dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 256, 128, 32) 128         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 256, 128, 32) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 128, 32) 4128        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 512, 256, 16) 2064        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 256, 32) 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 512, 256, 32) 128         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512, 256, 32) 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 512, 256, 16) 2064        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512, 256, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 512, 256, 16) 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 512, 256, 16) 1040        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 512, 256, 1)  17          conv2d_17[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 963,441\n",
      "Trainable params: 961,265\n",
      "Non-trainable params: 2,176\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_metrics = ['accuracy', metrics.MeanIoU(num_classes=2)]\n",
    "unet.compile(optimizer='adam', \n",
    "             loss = binary_crossentropy, \n",
    "             metrics = model_metrics,\n",
    "             callbacks=[unet_cb]\n",
    "             )\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vh07lbaSTgEo"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iMXoBbFw8Fs"
   },
   "outputs": [],
   "source": [
    "#estimated..TODO load sample of images, sum to scalar and divid by image size, \n",
    "#get average over image set \n",
    "class_weights = {0.0: 1, 1.0: 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "colab_type": "code",
    "id": "8LjVIpMzTiqp",
    "outputId": "21545f83-8106-4c4f-a30e-422bb94d79b0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-61b1f30f8878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m unet_history = unet.fit_generator(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_generator_ffd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_train_samples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator_ffd' is not defined"
     ]
    }
   ],
   "source": [
    "unet_history = unet.fit_generator(\n",
    "    train_generator_ffd, \n",
    "    steps_per_epoch=int(total_train_samples/batch_size),\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    #validation_data=vdate_generator,\n",
    "    #validation_steps=int(total_vdate_samples/batch_size),\n",
    "    callbacks = [unet_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "colab_type": "code",
    "id": "6Ehl03DJVTOy",
    "outputId": "2ffb16b9-90fe-4d0a-93c0-0d9bb0521758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-216:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/data_utils.py\", line 742, in _run\n",
      "    sequence = list(range(len(self.sequence)))\n",
      "  File \"<ipython-input-228-8bb6382199a7>\", line 7, in __len__\n",
      "    return int(np.ceil(len(self.generators[0]) / float(self.batch_size)))\n",
      "AttributeError: 'MergedGenerators' object has no attribute 'batch_size'\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-e51bb61c06f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#validation_data=vdate_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#validation_steps=int(total_vdate_samples/batch_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     callbacks = [cp])\n\u001b[0m",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unet_history = unet.fit_generator(\n",
    "    train_merged_generator, \n",
    "    steps_per_epoch=int(total_train_samples/batch_size),\n",
    "    epochs=n_epochs,\n",
    "    verbose=1,\n",
    "    #validation_data=vdate_generator,\n",
    "    #validation_steps=int(total_vdate_samples/batch_size),\n",
    "    callbacks = [cp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwqwh94HN9rn"
   },
   "source": [
    "# Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxvu7e-AN_ZB"
   },
   "outputs": [],
   "source": [
    "def plot_segmentation():\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "    \n",
    "    ax1.imshow(im, interpolation='none')\n",
    "    ax1.set_title(\"Test Image\")\n",
    "\n",
    "    ax2.imshow(im, interpolation='none')\n",
    "    ax2.imshow(mask_true, 'green', interpolation='none', alpha=.2)\n",
    "    ax2.set_title(\"True Mask\")\n",
    "\n",
    "    ax3.imshow(im, interpolation='none')\n",
    "    ax3.imshow(true_mask, 'green', interpolation='none', alpha=0.2)\n",
    "    ax3.set_title(\"Predicted Mask\")\n",
    "\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Unet Model for Semantic Segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
