{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet Model for Semantic Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y5Eyt_8WTMg",
        "colab_type": "text"
      },
      "source": [
        "# Using U-net style architecture for semantic segmentation\n",
        "I will train a u-net-style architecture on the labeled city dataset from "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27pX0dRKUlUq",
        "colab_type": "text"
      },
      "source": [
        "### Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQVzCt6XUjam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 4\n",
        "target_size = (512,256)\n",
        "n_epochs = 25\n",
        "learning_rate = 0.000_001\n",
        "run_once, fix_masks = False, False # to load/unzip/fix data, fix masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_1OrbHAMN5k",
        "colab_type": "text"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RINwHYjearUG",
        "colab_type": "code",
        "outputId": "708cb643-2cec-45b3-c863-1bd3f9158623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# get colab status\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  IN_COLAB = False"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-qrlIDKa8x4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras import metrics \n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.layers import (Dense, Conv2D, Flatten, Dropout, \n",
        "MaxPooling2D, BatchNormalization, Conv2DTranspose, concatenate, Input)\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence # to fix 'imagedatagenerator has no shape' error\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os, zipfile, glob, shutil\n",
        "from math import ceil \n",
        "from PIL import Image\n",
        "from skimage import io, transform\n",
        "import imageio.core.util  # so can be patched\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg3xJNubNYXZ",
        "colab_type": "code",
        "outputId": "1e62751e-2051-417d-867d-b919748f1bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    os.chdir(r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/')\n",
        "else:\n",
        "    os.chdir(os.path.expanduser(r'~/Google Drive/thinkful/colab_datasets/sidewalk_data/'))\n",
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUZmgziJcCCv",
        "colab_type": "text"
      },
      "source": [
        "# Data Load\n",
        "### Cityscape Dataset\n",
        "This is a publically availible dataset from https://www.cityscapes-dataset.com/, though not downloadable without requesting a login which is granted based on email domain.\n",
        "Citation:  \n",
        ">Cvpr2016M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, “The Cityscapes Dataset for Semantic Urban Scene Understanding,” in Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [Bibtex]\n",
        "[main paper](https://www.cityscapes-dataset.com/wordpress/wp-content/papercite-data/pdf/cordts2016cityscapes.pdf) · [supplemental ](https://www.cityscapes-dataset.com/wordpress/wp-content/papercite-data/pdf/cordts2016cityscapes-supplemental.pdf)· [arxiv](http://arxiv.org/abs/1604.01685) · [CVF](http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmO_yxT5cBzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_once: # run once\n",
        "  os.chdir(r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/')\n",
        "  with zipfile.ZipFile(os.path.join(os.getcwd(), 'gtFine_trainvaltest.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./cityscape/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZyo7fHrNvLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Color for sidewalk in mask from cityscape website:\n",
        "sidewalk_color = np.array([244, 35, 232])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6fawQA4DC0e",
        "colab_type": "text"
      },
      "source": [
        "### downsample images and fix directory structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-I3mnGdj24R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def color_checker(color):\n",
        "    '''looks for color used to code sidewalks'''\n",
        "    return 1 if np.array_equal(color, sidewalk_color) else 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uo1u5iaDCUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize(path, destination):\n",
        "    '''\n",
        "    downsample images by 4x, \n",
        "    remove transparency layer, \n",
        "    save to flattened dir stuc\n",
        "    Args:\n",
        "        path = current directory\n",
        "        destination = destination for images\n",
        "    '''\n",
        "    dirs1 = np.array([os.path.join(path, x) for x in os.listdir(path)])\n",
        "    dirs = dirs1[[os.path.isdir(x) for x in dirs1]]\n",
        "    for direc in dirs:   # direc should be mask vs image folders\n",
        "        if not os.path.isdir(direc):\n",
        "            continue\n",
        "        subdirs = [os.path.join(direc, x) for x in os.listdir(direc)]\n",
        "        for subdir in subdirs:   # subdir should be train, val, test\n",
        "            subdirs2 = [os.path.join(subdir, x) for x in os.listdir(subdir)]\n",
        "            for subdir2 in subdirs2:   # subdir2 is city folders\n",
        "                items = os.listdir(subdir2)\n",
        "                for item in items:\n",
        "                    if not os.path.isfile(os.path.join(subdir2,item)):\n",
        "                        continue\n",
        "                    #skip json and noncolor masks\n",
        "                    if item.rfind('gtFine_color.png') == -1 \\\n",
        "                    and item.rfind('leftImg8bit.png') == -1:\n",
        "                        continue\n",
        "                    #if item.rfind('leftImg8bit.png') != -1:  # remove if doing images\n",
        "                    #    continue\n",
        "                    destin2 = os.path.join(destination, subdir[len(common_path)+1:])\n",
        "                    f, _ = os.path.splitext(item)\n",
        "                    if not os.path.exists(destin2):\n",
        "                        os.makedirs(destin2)\n",
        "                    destin3 = os.path.join(destin2, f+'_resized.png')\n",
        "                    ###########WARNING\n",
        "                    #ADDED transfomation for masks. IF running \n",
        "                    #from scratch, add logic to treat masks vs images separately\n",
        "                    ###############\n",
        "                    #filename = os.path.join(subdir2, item)\n",
        "                    #im = io.imread(filename)\n",
        "                    #im = im[:,:,0:3]  # remove alpha channel\n",
        "                    #reduced_im = np.apply_along_axis(color_checker, 2, im).astype(np.intc)\n",
        "                    #reduced_im = transform.resize(reduced_im, \n",
        "                    #                            (256,512), \n",
        "                    #                            preserve_range=True, \n",
        "                    #                            anti_aliasing=False).astype(np.bool_).astype(np.intc)\n",
        "                    #io.imsave(destin3, reduced_im) \n",
        "\n",
        "\n",
        "\n",
        "                    im = Image.open(os.path.join(subdir2, item))\n",
        "                    if im.mode == \"RGBA\":\n",
        "                        im = im.convert('RGB')\n",
        "                    im = im.resize((512,256), Image.BICUBIC)\n",
        "                    im.save(destin3, format='PNG')\n",
        "                    im.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjQJpIMvdxud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8a77f4d3-5b5f-4efa-cbc4-a38593bb5daa"
      },
      "source": [
        "#switch to cloud dir\n",
        "if IN_COLAB:\n",
        "    data_mount = r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/'  \n",
        "    destin = r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/'\n",
        "else:\n",
        "    data_mount = os.path.expanduser(\n",
        "        r'~/Google Drive/thinkful/colab_datasets/sidewalk_data/public_data/')\n",
        "    destin = os.path.expanduser(\n",
        "        r\"~\\Documents\\GitHub\\sidewalks_deep-learning\\cityscape_reduced\")\n",
        "leftimages_path = os.path.join(data_mount, r'cityscape/leftImg8bit/') # no leading /\n",
        "mask_path = os.path.join(data_mount, r'cityscape/gtFine/')\n",
        "common_path = os.path.join(data_mount, r'cityscape')\n",
        "image_dest_path = os.path.join(destin, r'leftImg8bit')\n",
        "mask_dest_path = os.path.join(destin, r'sidewalk_only_masks')\n",
        "os.chdir(common_path)\n",
        "!pwd"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeFhHYZnDy8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if run_once:\n",
        "  resize(common_path, destin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1kRBlCrapN1",
        "colab_type": "text"
      },
      "source": [
        "### Adjust masks to only separate sidewalk vs non-sidewalk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYJdfnIAtPqW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "745375d0-6f52-4e4e-9cbb-63fc48fca6e5"
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O87DXqZVL201",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Requires both patching library and regular filterwarnings to avoid \n",
        "#warnings about low resolution!\n",
        "def silence_imageio_warning(*args, **kwargs):\n",
        "    pass\n",
        "imageio.core.util._precision_warn = silence_imageio_warning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mSpwws8UaRK",
        "colab_type": "text"
      },
      "source": [
        "#### create dataframe of mask files to operate on and intended output filename"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19SCyIIcXBPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "common_path_new = r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW3r9aqRUBx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(mask_path)\n",
        "fix_masks=True\n",
        "if fix_masks:\n",
        "    file_list = ! find . -name \"*_gtFine_color.png\"\n",
        "    mask_fixes_df = pd.DataFrame([path.split(\"/\") for path in file_list])\n",
        "    mask_fixes_df.columns = ['dot', 'subset', 'city', 'filename']\n",
        "    mask_fixes_df['dest_path'] = common_path_new + r'/sidewalk_only_masks/' + \\\n",
        "    mask_fixes_df.subset + '/folder0/' + mask_fixes_df.filename\n",
        "    mask_fixes_df['old_path'] = file_list\n",
        "    mask_fixes_df['old_path'] = mask_fixes_df['old_path'].apply(lambda x: mask_path + x[2:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF8Xz-lXMRth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_extant(path):\n",
        "    return True if os.path.exists(path) else False\n",
        "mask_fixes_df['already_fixed'] = mask_fixes_df.dest_path.apply(remove_extant)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxV5ph9pP10m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "fef03034-7194-4d07-8e59-4b18a2d792ca"
      },
      "source": [
        "mask_fixes_df.head()"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dot</th>\n",
              "      <th>subset</th>\n",
              "      <th>city</th>\n",
              "      <th>filename</th>\n",
              "      <th>dest_path</th>\n",
              "      <th>old_path</th>\n",
              "      <th>already_fixed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>.</td>\n",
              "      <td>train</td>\n",
              "      <td>jena</td>\n",
              "      <td>jena_000099_000019_gtFine_color.png</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>.</td>\n",
              "      <td>train</td>\n",
              "      <td>jena</td>\n",
              "      <td>jena_000083_000019_gtFine_color.png</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.</td>\n",
              "      <td>train</td>\n",
              "      <td>jena</td>\n",
              "      <td>jena_000012_000019_gtFine_color.png</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.</td>\n",
              "      <td>train</td>\n",
              "      <td>jena</td>\n",
              "      <td>jena_000026_000019_gtFine_color.png</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.</td>\n",
              "      <td>train</td>\n",
              "      <td>jena</td>\n",
              "      <td>jena_000029_000019_gtFine_color.png</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>/content/gdrive/My Drive/thinkful/colab_datase...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  dot subset  ...                                           old_path already_fixed\n",
              "0   .  train  ...  /content/gdrive/My Drive/thinkful/colab_datase...          True\n",
              "1   .  train  ...  /content/gdrive/My Drive/thinkful/colab_datase...          True\n",
              "2   .  train  ...  /content/gdrive/My Drive/thinkful/colab_datase...          True\n",
              "3   .  train  ...  /content/gdrive/My Drive/thinkful/colab_datase...          True\n",
              "4   .  train  ...  /content/gdrive/My Drive/thinkful/colab_datase...          True\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-rQEvT9XQmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_reduce(old_file, new_file):\n",
        "    '''reduces images from rgb to b/w mask'''\n",
        "    im = io.imread(old_file)\n",
        "    im = im[:,:,0:3]  # remove alpha channel\n",
        "    reduced_im = np.apply_along_axis(color_checker, 2, im).astype(np.intc)\n",
        "    reduced_im = transform.resize(reduced_im, \n",
        "                                  (256,512), \n",
        "                                  preserve_range=True, \n",
        "                                  anti_aliasing=False).astype(np.bool_).astype(np.intc)\n",
        "    #all black images throw error\n",
        "    try:\n",
        "        io.imsave(new_file, reduced_im) \n",
        "    except ValueError:\n",
        "        io.imsave(new_file, reduced_im.astype('float64'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I--D9TvUlhdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if fix_masks:\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        warnings.filterwarnings('ignore')\n",
        "        #apply fix to mask file w/o extant target\n",
        "        mask_fixes_df[mask_fixes_df.already_fixed==False]\\\n",
        "            .apply(lambda x: mask_reduce(x.old_path, x.dest_path), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIbz-k-aIpt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(destin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkj0QI8NWybp",
        "colab_type": "text"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QsDAxU1dZwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_locs(subset, im_path, add_folder0):    \n",
        "    '''grab name of every image \n",
        "    and store in dataframe with subset eg. train, test'''\n",
        "    df = pd.DataFrame(columns=['img_name', 'subset'])\n",
        "    os.chdir(im_path)   # needed despite lack of reference to curpath\n",
        "    img_paths = os.listdir(im_path) \n",
        "    if subset == 'img':\n",
        "        search_for = '*/*'\n",
        "    elif subset == 'mask':   \n",
        "        search_for = '*/*_gtFine_color.png'\n",
        "    else:\n",
        "        raise NameError(\"subset should be 'img' or 'mask'\")\n",
        "    if add_folder0:\n",
        "        im_paths = [os.path.join(im, 'folder0') for im in im_paths]\n",
        "    for subsetdir in img_paths:\n",
        "        img_files = [x[len(subsetdir)+1:] for x in glob.glob(os.path.join(subsetdir, search_for))]\n",
        "        img_files_s = pd.Series(img_files)\n",
        "        new_df = None\n",
        "        new_df = pd.DataFrame(columns=['img_name', 'subset'])\n",
        "        new_df['img_name'] = img_files_s\n",
        "        new_df['subset'].fillna(subsetdir, inplace=True)\n",
        "        df = df.append(new_df)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkAAKtaSQTMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_name(name):\n",
        "  '''removes _gtFine_color.png or _leftImg8bit.png'''\n",
        "  which = name.rfind('_leftImg8bit.png')\n",
        "  if which == -1:\n",
        "    which = '_gtFine_color.png'\n",
        "  else:\n",
        "    which =  '_leftImg8bit.png'\n",
        "  loc=name.rfind(which)\n",
        "  return name[:loc]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArIMjgFz-uz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = get_image_locs('img', leftimages_path, False)\n",
        "mask_df = get_image_locs('mask', mask_path, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSnp4W4JPmNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df['common_name'] = data_df.img_name.apply(clean_name)\n",
        "mask_df['common_name'] = mask_df.img_name.apply(clean_name)\n",
        "data_full_df = None\n",
        "data_full_df = pd.merge(left=data_df, right=mask_df, \n",
        "                        how='inner', on='common_name',\n",
        "                        sort=False, \n",
        "                        suffixes=('_data', '_mask'))\\\n",
        "               [['img_name_data', 'subset_data', 'img_name_mask']]\n",
        "data_full_df.columns = ['img_name', 'subset_data', 'mask_name']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpvr4AWeWyIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImgLoadGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    parent Sequence object generates data batches\n",
        "    \"\"\"\n",
        "    def __init__(self, data, batch_size, data_path, subset, widthxheight): \n",
        "        self.data = data[data.subset_data == subset]\n",
        "        self.data_path = data_path\n",
        "        self.batch_size = batch_size\n",
        "        self.image_list = data.img_name.to_list()\n",
        "        self.mask_list = data.mask_name.to_list()\n",
        "        self.subset = subset\n",
        "        self.w_h = widthxheight\n",
        "       \n",
        "    def __len__(self):\n",
        "      \"\"\"last batch usually smaller\"\"\"\n",
        "      return int(np.ceil(len(self.image_list) / float(self.batch_size)))\n",
        "    def decode_img(self, file_path):\n",
        "        img = tf.io.read_file(file_path)\n",
        "        # convert the compressed string to a 3D uint8 tensor\n",
        "        img = tf.image.decode_png(img, channels=3)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)  # to [0,1] rng\n",
        "        return tf.image.resize(img, self.w_h)\n",
        "    def process_path(self, file_path):\n",
        "        # load the raw data from the file as a string\n",
        "\n",
        "        return img, label\n",
        "    def get_batch(self, idx, path_list, img_or_mask):\n",
        "        # Fetch a batch of images from a list of paths\n",
        "        if img_or_mask == 'img':\n",
        "            path_l = os.path.join(self.data_path, \"leftImg8bit\", self.subset)\n",
        "            col_slice = 0\n",
        "        else:\n",
        "            path_l = os.path.join(self.data_path, \"gtFine\", self.subset)\n",
        "            col_slice = 2\n",
        "        slice_start, slice_end = idx * self.batch_size, (1 + idx) * self.batch_size\n",
        "        im_set = self.data.iloc[slice_start:slice_end,col_slice].values\n",
        "        return np.array([self.decode_img(os.path.join(path_l, im)) for im in im_set])\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.get_batch(idx, self.image_list, 'img')\n",
        "        batch_y = self.get_batch(idx, self.mask_list, 'mask')\n",
        "        return batch_x, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AzJ1S1Fbl-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_params = {'data':data_full_df,\n",
        "              'batch_size':8, \n",
        "              'data_path':common_path, \n",
        "              'widthxheight':target_size}\n",
        "train_generator = ImgLoadGenerator(subset='train', \n",
        "                                   **gen_params)\n",
        "vdate_generator = ImgLoadGenerator(subset='val',\n",
        "                                   **gen_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yinMtAVmKKIW",
        "colab_type": "text"
      },
      "source": [
        "### ImageDataGenerator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v8NXoyPahUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "413c3039-452e-4de9-99de-172ed99dfe6b"
      },
      "source": [
        "#create some subfolders for flow_for_directory\n",
        "subset = ['train', 'test', 'val']\n",
        "pathset = [mask_dest_path, image_dest_path]\n",
        "for sub in subset:\n",
        "    for path in pathset:\n",
        "        new_path = os.path.join(path, sub, 'folder0')\n",
        "        if not os.path.exists(new_path):\n",
        "            os.mkdir(new_path)\n",
        "        file_listing = os.listdir(os.path.join(path,sub))\n",
        "        file_listing = [file for file in file_listing if file!=new_path]        \n",
        "        for file in file_listing:\n",
        "            src=os.path.join(path,sub,file)\n",
        "            dst=os.path.join(new_path,file)\n",
        "            try:\n",
        "                shutil.move(src,dst)\n",
        "            except OSError:\n",
        "                pass"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/train/folder0\n",
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/leftImg8bit/train/folder0\n",
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/test/folder0\n",
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/leftImg8bit/test/folder0\n",
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/val/folder0\n",
            "/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/leftImg8bit/val/folder0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkiEpShXY90w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c8e940a-231b-4be1-c408-18885473aa48"
      },
      "source": [
        "os.path.join(mask_dest_path, 'train')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/public_data/cityscape_reduced/sidewalk_only_masks/train'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-KTOE105cID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#check all images have masks and vice-versa\n",
        "def check_im_mask_overlap(im_path, mask_path):\n",
        "    '''\n",
        "    Args:\n",
        "        how: pd.merge how=, as str. \n",
        "        inner for mask/image matchs, left for images WO mask, right mask WO img\n",
        "    '''\n",
        "    data_dest_df_img = get_image_locs('img', im_path, False)\n",
        "    mask_dest_df_mask = get_image_locs('mask', mask_path, False)\n",
        "    #print(data_dest_df_img[data_dest_df_img.duplicated(subset='img_name', keep='first')])\n",
        "    #print(mask_dest_df_mask[mask_dest_df_mask.duplicated(subset='img_name', keep='first')])\n",
        "    data_dest_df, mask_dest_df = pd.DataFrame(), pd.DataFrame()\n",
        "    data_dest_df['common_name'] = data_df.img_name.apply(clean_name)\n",
        "    mask_dest_df['common_name'] = mask_df.img_name.apply(clean_name)\n",
        "    data_full_df = None\n",
        "    data_full_df = pd.merge(left=data_df, right=mask_df, \n",
        "                            how='outer', on='common_name',\n",
        "                            indicator=True, sort=False, \n",
        "                            suffixes=('_data', '_mask'))\\\n",
        "                [['img_name_data', 'subset_data', 'img_name_mask', '_merge']]\n",
        "    data_full_df.columns = ['img_name', 'subset_data', 'mask_name', '_merge']\n",
        "    return data_full_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai5PjZ9371NZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 48
        },
        "outputId": "a1c1630f-59d3-4f96-eaa5-90b58e93751e"
      },
      "source": [
        "testererdf=check_im_mask_overlap(image_dest_path, mask_dest_path)\n",
        "testererdf[testererdf._merge == 'left']"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>subset_data</th>\n",
              "      <th>mask_name</th>\n",
              "      <th>_merge</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [img_name, subset_data, mask_name, _merge]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_JeKjS2saGN",
        "colab_type": "text"
      },
      "source": [
        "### combined generators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eio4oJwQsSg2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-PcSznXcFG7",
        "colab_type": "code",
        "outputId": "faaf22cb-8916-4a1e-b243-1a686a2220ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "mask_datagen = ImageDataGenerator(rescale=1./255)\n",
        "seed=42\n",
        "ffd_params = dict(class_mode=None,\n",
        "                  target_size=target_size,\n",
        "                  seed=seed,\n",
        "                  batch_size=batch_size,\n",
        "                  )\n",
        "image_generator = image_datagen.flow_from_directory(\n",
        "    os.path.join(image_dest_path, r'train'),\n",
        "    **ffd_params)\n",
        "\n",
        "mask_generator = mask_datagen.flow_from_directory(\n",
        "    os.path.join(mask_dest_path, r'train'),\n",
        "    color_mode='grayscale',\n",
        "    **ffd_params)\n",
        "\n",
        "# combine generators into one which yields image and masks\n",
        "def combine_generator(gen1, gen2):\n",
        "    while True:\n",
        "        yield(next(gen1), next(gen2))\n",
        "\n",
        "train_generator_ffd = combine_generator(image_generator, mask_generator)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2975 images belonging to 1 classes.\n",
            "Found 352 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z-vruN0FuVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Using Sequence to combine generators\n",
        "class MergedGenerators(Sequence):\n",
        "    def __init__(self, *generators):\n",
        "        self.generators = generators\n",
        "        # TODO add a check to verify that all generators have the same length\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.generators[0]) / float(self.batch_size)))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return [generator[index] for generator in self.generators]\n",
        "    \n",
        "    def \n",
        "    \n",
        "\n",
        "datagen_args = dict(class_mode=None,\n",
        "                  target_size=target_size,\n",
        "                  seed=seed,\n",
        "                  batch_size=batch_size,\n",
        "                  )\n",
        "\n",
        "train_merged_generator = MergedGenerators(image_generator, mask_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcdEvxeUFc_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "de03420a-31cc-4e35-91d9-235d3406e1ad"
      },
      "source": [
        "train_merged_generator()"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-230-f6e2d7c91dd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_merged_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'MergedGenerators' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZj1nOBdcGI-",
        "colab_type": "text"
      },
      "source": [
        "# Modeling\n",
        "I will build a basic U-net model, train it on the cityscape public dataset, and use that model & weights on the Denver dataset I have built. \n",
        "\n",
        "Model structure reference [here](http://cs230.stanford.edu/files_winter_2018/projects/6937642.pdf)\n",
        "\n",
        "Next, I will use the frozen weights from the first have of the model, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fRThv5kTFGT",
        "colab_type": "text"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gynMScrP_TBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_shape = (*target_size, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Vg7YryWNue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use function for the blocks of the network\n",
        "def encoder_builder(input_, filters,\n",
        "                         activ='relu', kernel=(3,3), \n",
        "                         drop=.5, pad='same', kern_init='he_uniform'):\n",
        "  kwargs = {'filters': filters, 'activation': activ, 'kernel_size': kernel, \n",
        "       'padding': pad, 'kernel_initializer': kern_init}\n",
        "  x = Conv2D(**kwargs)(input_)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(drop)(x)\n",
        "  x = Conv2D(**kwargs)(x)\n",
        "  encoder = Dropout(drop)(x)\n",
        "  pooled = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(encoder)\n",
        "  return encoder, pooled\n",
        "\n",
        "def decoder_builder(input_, skip, filters, \n",
        "                    activ='relu', kernel=(2,2),\n",
        "                    drop=.5, pad='same', kern_init='he_uniform',\n",
        "                    ):\n",
        "  kwargs = {'filters': filters, 'activation': activ, 'kernel_size': kernel, \n",
        "       'padding': pad, 'kernel_initializer': kern_init} \n",
        "  x = Conv2DTranspose(**kwargs, strides=(2,2))(input_)\n",
        "  x = concatenate([x, skip], axis=-1)  # note axis is *-*1\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(drop)(x)\n",
        "  x = Conv2D(**kwargs)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(drop)(x)\n",
        "  x = Conv2D(**kwargs)(x)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqVLXyW3x94R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer = Input(shape=img_shape)\n",
        "encoder1, pooled1 = encoder_builder(input_layer, filters=16)  # return (128x128x32)\n",
        "encoder2, pooled2 = encoder_builder(pooled1, filters=32)  # return (64x64x364)\n",
        "encoder3, pooled3 = encoder_builder(pooled2, filters=64)  # return (32x32x128)\n",
        "encoder4, pooled4 = encoder_builder(pooled3, filters=128)  # return (16x16x256)\n",
        "#encoder_builder(pooled4, filters=512)  # return (8x8x512)\n",
        "middle, middle_pool = encoder_builder(pooled4, filters=128)  # return (4x4x1024)\n",
        "#decoder512 = decoder_builder(middle, skip=encoder5, filters=512)\n",
        "decoder256 = decoder_builder(middle, skip=encoder4, filters=128)\n",
        "decoder128 = decoder_builder(decoder256, skip=encoder3, filters=64)\n",
        "decoder64 = decoder_builder(decoder128, skip=encoder2, filters=32)\n",
        "decoder32 = decoder_builder(decoder64, skip=encoder1, filters=16)\n",
        "out_layer = Conv2D(filters=1, kernel_size=(1, 1), \n",
        "                   activation='sigmoid')(decoder32)\n",
        "\n",
        "####copied code\n",
        "#final_conv_out = Conv2D(n_classes, 1, 1)(conv9)\n",
        "\n",
        "#x = Reshape((2, rows*cols))(final_conv_out)\n",
        "#x = Permute((2,1))(x)\n",
        "\n",
        "# seg is a pixelwise probability vector sized (batch_size, rows*cols, n_classes)\n",
        "#seg = Activation(\"softmax\")(x)                   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6xAW0lHDuCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unet = models.Model(inputs=[input_layer], outputs=[out_layer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsRDy7X_EzUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_metrics = ['accuracy', metrics.MeanIoU(num_classes=2)]]\n",
        "unet.compile(optimizer='adam', \n",
        "             loss = binary_crossentropy, \n",
        "             metrics = model_metrics)\n",
        "unet.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWDSkQxsa-gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(unet, show_shapes=True, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh07lbaSTgEo",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ICDJMRUGj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = img_shape\n",
        "total_train_samples = data_full_df[data_full_df.subset_data == 'train'].shape[0]\n",
        "total_vdate_samples = data_full_df[data_full_df.subset_data == 'val'].shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whoa9jSmM0Jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model_path = r'/content/gdrive/My Drive/thinkful/colab_datasets/sidewalk_data/models/weights.hdf5'\n",
        "cp = ModelCheckpoint(filepath=save_model_path, \n",
        "                                        monitor=model_metrics, \n",
        "                                        save_best_only=True, \n",
        "                                        verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LjVIpMzTiqp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "7dad34e3-4262-4686-e049-a88740b5f838"
      },
      "source": [
        "unet_history = unet.fit_generator(\n",
        "    train_generator_ffd, \n",
        "    steps_per_epoch=int(total_train_samples/batch_size),\n",
        "    epochs=n_epochs,\n",
        "    verbose=1,\n",
        "    #validation_data=vdate_generator,\n",
        "    #validation_steps=int(total_vdate_samples/batch_size),\n",
        "    callbacks = [cp])"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "731/743 [============================>.] - ETA: 10s - loss: 0.2083 - accuracy: 0.9392"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-383da63fd598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#validation_data=vdate_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#validation_steps=int(total_vdate_samples/batch_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     callbacks = [cp])\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2532\u001b[0m       \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2534\u001b[0;31m         \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    675\u001b[0m                      \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m                      \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
            "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 3 input samples and 4 target samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ehl03DJVTOy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "outputId": "2ffb16b9-90fe-4d0a-93c0-0d9bb0521758"
      },
      "source": [
        "unet_history = unet.fit_generator(\n",
        "    train_merged_generator, \n",
        "    steps_per_epoch=int(total_train_samples/batch_size),\n",
        "    epochs=n_epochs,\n",
        "    verbose=1,\n",
        "    #validation_data=vdate_generator,\n",
        "    #validation_steps=int(total_vdate_samples/batch_size),\n",
        "    callbacks = [cp])"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-216:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/data_utils.py\", line 742, in _run\n",
            "    sequence = list(range(len(self.sequence)))\n",
            "  File \"<ipython-input-228-8bb6382199a7>\", line 7, in __len__\n",
            "    return int(np.ceil(len(self.generators[0]) / float(self.batch_size)))\n",
            "AttributeError: 'MergedGenerators' object has no attribute 'batch_size'\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-231-e51bb61c06f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#validation_data=vdate_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#validation_steps=int(total_vdate_samples/batch_size),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     callbacks = [cp])\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator)\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}